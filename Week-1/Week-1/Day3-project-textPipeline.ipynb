{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44104998-c309-44eb-a568-d957bc54532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ca33f5-0be2-4d1b-98e7-7f9736fe1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample earnings call transcript\n",
    "sample_transcript = \"\"\"\n",
    "Good afternoon, everyone. This is Tim Cook, CEO of TechCorp.\n",
    "We're pleased to report strong Q4 results with revenue of $90.1 billion.\n",
    "Our cloud segment saw tremendous growth of 35% year-over-year.\n",
    "We now have 2.5 billion monthly active users across our platforms.\n",
    "Despite macro challenges, our AI initiatives are showing promising results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1f45-d0dc-4404-8956-e587df860e55",
   "metadata": {},
   "source": [
    "### Define the TechEarningsAnalyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792e9f10-f4ba-47ae-8e8e-27a43b83707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechEarningsAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Custom financial metrics patterns\n",
    "        self.metric_patterns = {\n",
    "            'revenue': r'\\$?\\d+\\.?\\d*\\s*(billion|million|B|M)',\n",
    "            'growth': r'\\d+\\.?\\d*\\s*%',\n",
    "            'margin': r'\\d+\\.?\\d*\\s*%\\s*margin',\n",
    "            'users': r'\\d+\\.?\\d*\\s*(million|billion)?\\s*(monthly active users|MAU|daily active users|DAU)',\n",
    "        }\n",
    "        \n",
    "        # Tech-specific keywords\n",
    "        self.tech_segments = {\n",
    "            'cloud': ['cloud', 'aws', 'azure', 'gcp'],\n",
    "            'ai': ['ai', 'artificial intelligence', 'machine learning', 'ml'],\n",
    "            'hardware': ['iphone', 'device', 'hardware', 'macbook', 'surface'],\n",
    "            'services': ['subscription', 'saas', 'services'],\n",
    "        }\n",
    "        \n",
    "        self.exec_titles = ['CEO', 'CTO', 'CFO', 'COO', 'President', 'Director']\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and normalize the transcript text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "        \n",
    "    def extract_metrics(self, text):\n",
    "        \"\"\"Extract key financial and operational metrics\"\"\"\n",
    "        metrics = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for metric_type, pattern in self.metric_patterns.items():\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                metrics[metric_type].append(match.group())\n",
    "        \n",
    "        return dict(metrics)\n",
    "        \n",
    "    def analyze_segment_mentions(self, text):\n",
    "        \"\"\"Analyze mentions and context of different tech segments\"\"\"\n",
    "        segment_analysis = defaultdict(lambda: {'mentions': 0, 'context': []})\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for segment, keywords in self.tech_segments.items():\n",
    "            for sent in doc.sents:\n",
    "                sent_text = sent.text.lower()\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sent_text:\n",
    "                        segment_analysis[segment]['mentions'] += 1\n",
    "                        segment_analysis[segment]['context'].append(sent.text)\n",
    "        \n",
    "        return dict(segment_analysis)\n",
    "        \n",
    "    def extract_speakers(self, text):\n",
    "        \"\"\"Extract speakers and their quotes\"\"\"\n",
    "        speakers = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    next_words = ent.doc[ent.end:ent.end + 3].text\n",
    "                    for title in self.exec_titles:\n",
    "                        if title in next_words:\n",
    "                            speakers[f\"{ent.text} ({title})\"].append(sent.text)\n",
    "        \n",
    "        return dict(speakers)\n",
    "\n",
    "    def sentiment_analysis(self, text):\n",
    "        \"\"\"Analyze sentiment in the earnings call\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        positive_words = ['growth', 'increase', 'improved', 'strong', 'success']\n",
    "        negative_words = ['decline', 'decrease', 'challenge', 'difficult', 'loss']\n",
    "        \n",
    "        sentiment = {\n",
    "            'positive_sentences': [],\n",
    "            'negative_sentences': [],\n",
    "            'overall_tone': 'neutral'\n",
    "        }\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            pos_count = sum(1 for word in positive_words if word in sent_text)\n",
    "            neg_count = sum(1 for word in negative_words if word in sent_text)\n",
    "            \n",
    "            if pos_count > neg_count:\n",
    "                sentiment['positive_sentences'].append(sent.text)\n",
    "            elif neg_count > pos_count:\n",
    "                sentiment['negative_sentences'].append(sent.text)\n",
    "        \n",
    "        sentiment['overall_tone'] = 'positive' if len(sentiment['positive_sentences']) > len(sentiment['negative_sentences']) else 'negative'\n",
    "        return sentiment\n",
    "        \n",
    "    def generate_report(self, text):\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        report = {\n",
    "            'metrics': self.extract_metrics(text),\n",
    "            'segment_analysis': self.analyze_segment_mentions(text),\n",
    "            'speakers': self.extract_speakers(text),\n",
    "            'sentiment': self.sentiment_analysis(text)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    def save_report(self, report, filename):\n",
    "        \"\"\"Save the analysis report to Excel\"\"\"\n",
    "        with pd.ExcelWriter(filename) as writer:\n",
    "            pd.DataFrame(report['metrics']).to_excel(writer, sheet_name='Metrics')\n",
    "            pd.DataFrame(report['segment_analysis']).to_excel(writer, sheet_name='Segment Analysis')\n",
    "            pd.DataFrame(report['speakers']).to_excel(writer, sheet_name='Speakers')\n",
    "            pd.DataFrame(report['sentiment']).to_excel(writer, sheet_name='Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046b986-684a-48e6-b2c9-9f25a2d918b2",
   "metadata": {},
   "source": [
    "### Implement Metric Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4854d3c-80f7-4bc0-bd6d-fdbdeadd3532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Metrics:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m metrics \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mextract_metrics(sample_transcript)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop - Brendanâ€™s MacBook Air/nvda-certification/torch-ml-312/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Desktop - Brendanâ€™s MacBook Air/nvda-certification/torch-ml-312/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop - Brendanâ€™s MacBook Air/nvda-certification/torch-ml-312/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Desktop - Brendanâ€™s MacBook Air/nvda-certification/torch-ml-312/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "def extract_metrics(self, text):\n",
    "        \"\"\"Extract key financial and operational metrics\"\"\"\n",
    "        metrics = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for metric_type, pattern in self.metric_patterns.items():\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                metrics[metric_type].append(match.group())\n",
    "        \n",
    "        return dict(metrics)\n",
    "\n",
    "# Test metric extraction\n",
    "analyzer = TechEarningsAnalyzer()\n",
    "metrics = analyzer.extract_metrics(sample_transcript)\n",
    "print(\"Extracted Metrics:\")\n",
    "pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d035185-02b4-4aa0-85e2-a2d4149e88a3",
   "metadata": {},
   "source": [
    "### Implement Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d5b31-d1c5-403f-a71b-f7b880afba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segment_mentions(self, text):\n",
    "        \"\"\"Analyze mentions and context of different tech segments\"\"\"\n",
    "        segment_analysis = defaultdict(lambda: {'mentions': 0, 'context': []})\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for segment, keywords in self.tech_segments.items():\n",
    "            for sent in doc.sents:\n",
    "                sent_text = sent.text.lower()\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sent_text:\n",
    "                        segment_analysis[segment]['mentions'] += 1\n",
    "                        segment_analysis[segment]['context'].append(sent.text)\n",
    "        \n",
    "        return dict(segment_analysis)\n",
    "\n",
    "# Test segment analysis\n",
    "segments = analyzer.analyze_segment_mentions(sample_transcript)\n",
    "print(\"\\nSegment Analysis:\")\n",
    "pd.DataFrame(segments).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaebbf2-c4ba-4dd6-a1cd-021419c28a42",
   "metadata": {},
   "source": [
    "### Implement Speaker and Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80feace-7449-4c26-83dd-68fd18ae8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speakers(self, text):\n",
    "        \"\"\"Extract speakers and their quotes\"\"\"\n",
    "        speakers = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    next_words = ent.doc[ent.end:ent.end + 3].text\n",
    "                    for title in self.exec_titles:\n",
    "                        if title in next_words:\n",
    "                            speakers[f\"{ent.text} ({title})\"].append(sent.text)\n",
    "        \n",
    "        return dict(speakers)\n",
    "\n",
    "    def sentiment_analysis(self, text):\n",
    "        \"\"\"Analyze sentiment in the earnings call\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        positive_words = ['growth', 'increase', 'improved', 'strong', 'success']\n",
    "        negative_words = ['decline', 'decrease', 'challenge', 'difficult', 'loss']\n",
    "        \n",
    "        sentiment = {\n",
    "            'positive_sentences': [],\n",
    "            'negative_sentences': [],\n",
    "            'overall_tone': 'neutral'\n",
    "        }\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            pos_count = sum(1 for word in positive_words if word in sent_text)\n",
    "            neg_count = sum(1 for word in negative_words if word in sent_text)\n",
    "            \n",
    "            if pos_count > neg_count:\n",
    "                sentiment['positive_sentences'].append(sent.text)\n",
    "            elif neg_count > pos_count:\n",
    "                sentiment['negative_sentences'].append(sent.text)\n",
    "        \n",
    "        sentiment['overall_tone'] = 'positive' if len(sentiment['positive_sentences']) > len(sentiment['negative_sentences']) else 'negative'\n",
    "        return sentiment\n",
    "\n",
    "# Test speaker and sentiment analysis\n",
    "speakers = analyzer.extract_speakers(sample_transcript)\n",
    "sentiment = analyzer.sentiment_analysis(sample_transcript)\n",
    "\n",
    "print(\"\\nSpeaker Analysis:\")\n",
    "pd.DataFrame(speakers)\n",
    "\n",
    "print(\"\\nSentiment Analysis:\")\n",
    "print(f\"Overall Tone: {sentiment['overall_tone']}\")\n",
    "print(\"\\nPositive Sentences:\")\n",
    "for sent in sentiment['positive_sentences']:\n",
    "    print(f\"- {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3c15a-3ab9-4509-a171-0f4976423d7d",
   "metadata": {},
   "source": [
    "### Generate Complete Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b062288-7d0b-41ff-b871-7023f81c94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(self, text):\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        report = {\n",
    "            'metrics': self.extract_metrics(text),\n",
    "            'segment_analysis': self.analyze_segment_mentions(text),\n",
    "            'speakers': self.extract_speakers(text),\n",
    "            'sentiment': self.sentiment_analysis(text)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Generate complete report\n",
    "report = analyzer.generate_report(sample_transcript)\n",
    "\n",
    "# Display results in a notebook-friendly way\n",
    "print(\"Complete Analysis Report:\")\n",
    "print(\"\\n1. Metrics:\")\n",
    "pd.DataFrame(report['metrics'])\n",
    "\n",
    "print(\"\\n2. Segment Analysis:\")\n",
    "pd.DataFrame(report['segment_analysis']).T\n",
    "\n",
    "print(\"\\n3. Speakers:\")\n",
    "pd.DataFrame(report['speakers'])\n",
    "\n",
    "print(\"\\n4. Sentiment:\")\n",
    "print(f\"Overall Tone: {report['sentiment']['overall_tone']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b30363c0-45f0-46ef-b3c9-c86fe5c81473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Report:\n",
      "\n",
      "1. Metrics:\n",
      "          revenue growth                             users\n",
      "0  $910.1 billion    35%  2.5 billion monthly active users\n",
      "1     2.5 billion   None                              None\n",
      "\n",
      "2. Segment Analysis:\n",
      "       mentions\n",
      "cloud         1\n",
      "ai            1\n",
      "\n",
      "3. Speakers:\n",
      "                       Tim Cook (CEO)\n",
      "0  This is Tim Cook, CEO of TechCorp.\n",
      "\n",
      "4. Sentiment:\n",
      "       tone  positive_count  negative_count\n",
      "0  positive               2               1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class TechEarningsAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Custom financial metrics patterns\n",
    "        self.metric_patterns = {\n",
    "            'revenue': r'\\$?\\d+\\.?\\d*\\s*(billion|million|B|M)',\n",
    "            'growth': r'\\d+\\.?\\d*\\s*%',\n",
    "            'margin': r'\\d+\\.?\\d*\\s*%\\s*margin',\n",
    "            'users': r'\\d+\\.?\\d*\\s*(million|billion)?\\s*(monthly active users|MAU|daily active users|DAU)',\n",
    "        }\n",
    "        \n",
    "        # Tech-specific keywords\n",
    "        self.tech_segments = {\n",
    "            'cloud': ['cloud', 'aws', 'azure', 'gcp'],\n",
    "            'ai': ['ai', 'artificial intelligence', 'machine learning', 'ml'],\n",
    "            'hardware': ['iphone', 'device', 'hardware', 'macbook', 'surface'],\n",
    "            'services': ['subscription', 'saas', 'services'],\n",
    "        }\n",
    "        \n",
    "        self.exec_titles = ['CEO', 'CTO', 'CFO', 'COO', 'President', 'Director']\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and normalize the transcript text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "        \n",
    "    def extract_metrics(self, text):\n",
    "        \"\"\"Extract key financial and operational metrics\"\"\"\n",
    "        metrics = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for metric_type, pattern in self.metric_patterns.items():\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                metrics[metric_type].append(match.group())\n",
    "        \n",
    "        # Convert lists to padded lists of equal length\n",
    "        max_length = max(len(v) for v in metrics.values()) if metrics else 0\n",
    "        for key in metrics:\n",
    "            metrics[key] = metrics[key] + [None] * (max_length - len(metrics[key]))\n",
    "            \n",
    "        return dict(metrics)\n",
    "        \n",
    "    def analyze_segment_mentions(self, text):\n",
    "        \"\"\"Analyze mentions and context of different tech segments\"\"\"\n",
    "        segment_analysis = defaultdict(lambda: {'mentions': 0, 'context': []})\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for segment, keywords in self.tech_segments.items():\n",
    "            for sent in doc.sents:\n",
    "                sent_text = sent.text.lower()\n",
    "                for keyword in keywords:\n",
    "                    if keyword in sent_text:\n",
    "                        segment_analysis[segment]['mentions'] += 1\n",
    "                        segment_analysis[segment]['context'].append(sent.text)\n",
    "        \n",
    "        return dict(segment_analysis)\n",
    "        \n",
    "    def extract_speakers(self, text):\n",
    "        \"\"\"Extract speakers and their quotes\"\"\"\n",
    "        speakers = defaultdict(list)\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    next_words = ent.doc[ent.end:ent.end + 3].text\n",
    "                    for title in self.exec_titles:\n",
    "                        if title in next_words:\n",
    "                            speakers[f\"{ent.text} ({title})\"].append(sent.text)\n",
    "        \n",
    "        return dict(speakers)\n",
    "\n",
    "    def sentiment_analysis(self, text):\n",
    "        \"\"\"Analyze sentiment in the earnings call\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        positive_words = ['growth', 'increase', 'improved', 'strong', 'success']\n",
    "        negative_words = ['decline', 'decrease', 'challenge', 'difficult', 'loss']\n",
    "        \n",
    "        sentiment = {\n",
    "            'positive_sentences': [],\n",
    "            'negative_sentences': [],\n",
    "            'overall_tone': 'neutral'\n",
    "        }\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            pos_count = sum(1 for word in positive_words if word in sent_text)\n",
    "            neg_count = sum(1 for word in negative_words if word in sent_text)\n",
    "            \n",
    "            if pos_count > neg_count:\n",
    "                sentiment['positive_sentences'].append(sent.text)\n",
    "            elif neg_count > pos_count:\n",
    "                sentiment['negative_sentences'].append(sent.text)\n",
    "        \n",
    "        sentiment['overall_tone'] = 'positive' if len(sentiment['positive_sentences']) > len(sentiment['negative_sentences']) else 'negative'\n",
    "        return sentiment\n",
    "        \n",
    "    def generate_report(self, text):\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        metrics = self.extract_metrics(text)\n",
    "        segments = self.analyze_segment_mentions(text)\n",
    "        speakers = self.extract_speakers(text)\n",
    "        sentiment_results = self.sentiment_analysis(text)\n",
    "        \n",
    "        return {\n",
    "            'metrics': pd.DataFrame(metrics),\n",
    "            'segment_analysis': pd.DataFrame({k: v['mentions'] for k, v in segments.items()}, index=['mentions']).T,\n",
    "            'speakers': pd.DataFrame(speakers),\n",
    "            'sentiment': pd.DataFrame({\n",
    "                'tone': [sentiment_results['overall_tone']],\n",
    "                'positive_count': [len(sentiment_results['positive_sentences'])],\n",
    "                'negative_count': [len(sentiment_results['negative_sentences'])]\n",
    "            })\n",
    "        }\n",
    "\n",
    "# Test the analyzer\n",
    "sample_transcript = \"\"\"\n",
    "Good afternoon, everyone. This is Tim Cook, CEO of TechCorp.\n",
    "We're pleased to report strong Q4 results with revenue of $910.1 billion.\n",
    "Our cloud segment saw tremendous growth of 35% year-over-year.\n",
    "We now have 2.5 billion monthly active users across our platforms.\n",
    "Despite macro challenges, our AI initiatives are showing promising results.\n",
    "\"\"\"\n",
    "\n",
    "# Create analyzer instance and generate report\n",
    "analyzer = TechEarningsAnalyzer()\n",
    "report = analyzer.generate_report(sample_transcript)\n",
    "\n",
    "# Print results\n",
    "print(\"Analysis Report:\")\n",
    "print(\"\\n1. Metrics:\")\n",
    "print(report['metrics'])\n",
    "\n",
    "print(\"\\n2. Segment Analysis:\")\n",
    "print(report['segment_analysis'])\n",
    "\n",
    "print(\"\\n3. Speakers:\")\n",
    "print(report['speakers'])\n",
    "\n",
    "print(\"\\n4. Sentiment:\")\n",
    "print(report['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a0c0e-6c1e-4919-beec-8577512322bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
