{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec440eb-eaa0-40c8-9ac0-28d5ae6249f8",
   "metadata": {},
   "source": [
    "### Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24269949-09bf-4c0c-9460-58faed30ab5d",
   "metadata": {},
   "source": [
    "#### 30,000 Foot View: \n",
    "+ Activation functions are crucial components of neural networks that determine whether a neuron should be activated based on its input. They **introduce non-linearity into the model**, enabling it to **learn complex patterns** in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3aebd3-8197-45e8-828a-b9395acc5811",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0aa45-80fd-4b2b-b9cb-abb18d386ca5",
   "metadata": {},
   "source": [
    "#### Importance:\n",
    "+ Most real-world data relationships are non-linear\n",
    "+ Activation functions allow neural networks to model these complex relationships rather than being restricted to linear transformations\n",
    "  \n",
    "##### Decision making\n",
    "+ They help the network decide which information is important and should be passed to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7c26f-aab3-4463-8676-e11e582335e7",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9ce2d-e958-4fd0-beab-fb9be6fcbe22",
   "metadata": {},
   "source": [
    "### Common Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a73bb-6cb7-46f0-8f66-3d58fd375420",
   "metadata": {},
   "source": [
    "#### 1. Binary Step\n",
    "+ Activates a neuron, if the input exceeds a certain threshold\n",
    "+ But, it cannot handle multi-class outputs, and it also has a gradient of 0, so cannot do back propagation\n",
    "\n",
    "#### 2. Linear Activation\n",
    "+ Outputs the input directly - identity function\n",
    "+ But, does not allow for complex mapping, as all layers collapse into 1 linear transformation\n",
    "\n",
    "#### 3. non-Linear Activation\n",
    "+ 1. ReLU (Rectified Linear Unit): Helps mitigate the vanishing gradient problem and is computationally efficient\n",
    "  2. Sigmoid Function: commonly used for binary classification problems\n",
    "  3. Softmax function: converts raw scores into probabilites for multi-class classification\n",
    "\n",
    "#### 4. Other non-Linear Activation\n",
    "+ Tanh (Hyperbolic Tangent): outputs values between -1 and 1, often in hidden layers\n",
    "+ Leaky ReLU: Allows a small gradient when the unit is inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10695806-92ca-40de-8189-9e591fe31ff2",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bd8d1-51b0-4d66-b67f-20f80f7497ef",
   "metadata": {},
   "source": [
    "#### Choosing the Right Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c0187-fabc-4f1d-9fdc-136878def628",
   "metadata": {},
   "source": [
    "+ For hidden layers, ReLU and its variants (like Leaky ReLU) are preferred due to their efficiency and effectiveness in training deep networks\n",
    "+ For output layers, the choice depends on the task:\n",
    "+ 1. Sigmoid for binary classification\n",
    "  2. Softmax for multi-class classification\n",
    "  3. Linear activation for regression tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed808e4-239b-4455-8581-70d7081435dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
