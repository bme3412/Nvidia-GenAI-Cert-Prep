{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1389b84-e9ce-4b29-ba34-7272eae37ccb",
   "metadata": {},
   "source": [
    "#### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc44445-d5cd-4488-8458-a9f4c40f0f71",
   "metadata": {},
   "source": [
    "##### Overview of the Encoder-Decoder architecture\n",
    "+ The encoder-decoder architecture in the Transformer model revolutionizes how we approach sequence-to-sequence tasks.\n",
    "+ The `encoder` creates **rich, contextual representations of the input data**, while the `decoder` uses these representations—along with its own past outputs—to **generate coherent and contextually accurate** sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41369ec-2381-4b99-b3a3-6df1931b3ef7",
   "metadata": {},
   "source": [
    "+ At its core, the encoder-decoder framework consists of two main components:\n",
    "    + 1. Encoder: Processes the input sequence and **transforms** it into a set of continuous representations (often referred to as \"hidden states\"); these representations capture the essential features and context of the input data.\n",
    "    + 2. Decoder: Uses the encoder's representations along with its own previous outputs to generate the target sequence; it **“decodes”** the information into the desired output, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11de5e-8a6e-4a1b-afcc-013f9b98afab",
   "metadata": {},
   "source": [
    "#### How the Transformer Implements the Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c075a7-d648-490f-a974-7ba83a40fdce",
   "metadata": {},
   "source": [
    "#### 1. Encoder: Understanding the Input\n",
    "Layer Structure:\n",
    "+ The Transformer encoder is composed of a stack of identical layers. Each layer has two primary sub-layers:\n",
    "\n",
    "1. Multi-Head Self-Attention: This mechanism allows the encoder to weigh the importance of different parts of the input sequence relative to each token. It helps capture dependencies regardless of their distance in the sequence.\n",
    "\n",
    "2. Feed-Forward Neural Network (FFN): A position-wise network that further processes the representations generated by the attention mechanism.\n",
    "\n",
    "##### Positional Encoding:\n",
    "+ Since the Transformer does not inherently encode sequence order (unlike recurrent networks), positional encodings are added to the input embeddings to provide information about the order of tokens.\n",
    "\n",
    "Layer Normalization and Residual Connections:\n",
    "+ Each sub-layer is followed by layer normalization and uses residual connections (i.e., adding the input of the sub-layer to its output) to facilitate training and stabilize gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8601e-69da-4cc7-88b1-a4dc0680294f",
   "metadata": {},
   "source": [
    "#### 2. Decoder: Generating the Output\n",
    "Layer Structure:\n",
    "+ Like the encoder, the decoder consists of a stack of layers. However, each decoder layer has three main sub-layers:\n",
    "\n",
    "##### 1. Masked Multi-Head Self-Attention: \n",
    "+ The “masking” ensures that the model cannot \"cheat\" by looking ahead at future tokens in the output sequence during training. This allows the decoder to generate the output one token at a time.\n",
    "\n",
    "##### 2. Encoder-Decoder (Cross) Attention: \n",
    "+ This layer enables the decoder to focus on relevant parts of the encoder's output. It computes attention scores between the decoder’s current state and all encoder outputs, effectively “translating” the input representation into the context needed for output generation.\n",
    "\n",
    "##### 3. Feed-Forward Neural Network (FFN): Like in the encoder, this network processes the outputs of the attention mechanisms.\n",
    "\n",
    "##### Positional Encoding and Residual Connections:\n",
    "+ Positional encodings are also used in the decoder to maintain information about the sequence order. Residual connections and layer normalization are similarly applied for stability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b1989-6a45-44eb-8024-784c8574b76d",
   "metadata": {},
   "source": [
    "#### Key Benefits of the Encoder-Decoder Framework in Transformers\n",
    "\n",
    "##### Parallel Processing:\n",
    "+ Unlike traditional RNN-based encoder-decoder models, the Transformer architecture allows the encoder and decoder to process sequences in parallel during training, significantly speeding up computation.\n",
    "\n",
    "##### Long-Range Dependencies:\n",
    "+ The self-attention mechanism within the encoder (and the cross-attention in the decoder) can capture relationships between tokens regardless of their position, making it highly effective for modeling long-range dependencies.\n",
    "\n",
    "##### Modularity and Flexibility:\n",
    "+ The clear separation between the encoder and decoder allows for flexible design choices. For example, one might pretrain the encoder on a large corpus and then fine-tune the decoder for a specific task, or vice versa.\n",
    "\n",
    "##### Scalability:\n",
    "+ With the self-attention mechanism, the Transformer can scale to handle very long sequences efficiently, as it is not limited by the sequential nature of recurrent architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8515a-1be6-4899-894b-72bf569e5619",
   "metadata": {},
   "source": [
    "### Practical Implications and Use Cases\n",
    "\n",
    "#### 1. Machine Translation:\n",
    "+ In tasks like translating text from one language to another, the encoder processes the source language sentence to capture its meaning, and the decoder then generates the corresponding sentence in the target language, guided by the context provided by the encoder.\n",
    "\n",
    "#### 2. Text Summarization:\n",
    "+ The encoder can distill the main points of a long document, and the decoder can produce a concise summary.\n",
    "\n",
    "#### 3. Question Answering:\n",
    "+The encoder might be used to comprehend a passage of text, and the decoder then generates an answer based on the information encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1caf3-2037-484d-95b8-9f121f4b96d3",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fc968-1ac5-436a-b50f-36aef9531be6",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Defining the Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2fb7cb-314b-4d67-a3f5-5e635b31db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding applied, shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: the dimension of embeddings.\n",
    "            max_len: the maximum length of sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a long enough matrix of positional encodings (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # apply sine to even indices in the array\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # apply cosine to odd indices in the array\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_length, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        seq_length = x.size(1)\n",
    "        x = x + self.pe[:, :seq_length]\n",
    "        return x\n",
    "\n",
    "# Quick test of positional encoding:\n",
    "d_model = 512\n",
    "pe = PositionalEncoding(d_model)\n",
    "dummy_input = torch.zeros(2, 10, d_model)  # (batch_size, seq_length, d_model)\n",
    "print(\"Positional Encoding applied, shape:\", pe(dummy_input).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fc263-5937-40be-85c0-bd4cb1433cdd",
   "metadata": {},
   "source": [
    "#### Define the Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f024eb-564b-4508-937d-efb4000bc488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input tensor of shape (seq_length, batch_size, d_model)\n",
    "            src_mask: optional mask for attention weights.\n",
    "            src_key_padding_mask: optional padding mask.\n",
    "        Returns:\n",
    "            Processed tensor of same shape.\n",
    "        \"\"\"\n",
    "        # Self-attention: each position attends to every other position.\n",
    "        attn_output, _ = self.self_attn(src, src, src,\n",
    "                                        attn_mask=src_mask,\n",
    "                                        key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        # Feed-forward network.\n",
    "        ff_output = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "# Quick test of the encoder block with dummy data:\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(d_model, nhead, dim_feedforward, dropout)\n",
    "dummy_src = torch.rand(seq_length, batch_size, d_model)  # (sequence length, batch size, d_model)\n",
    "encoder_output = encoder_block(dummy_src)\n",
    "print(\"Encoder output shape:\", encoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48647ed6-0a5f-4372-93e2-e319c3cfce5d",
   "metadata": {},
   "source": [
    "#### Define the Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4fa9c8-affb-4d56-912e-62d3dd2db312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: target sequence tensor (seq_length, batch_size, d_model)\n",
    "            memory: encoder output tensor (seq_length, batch_size, d_model)\n",
    "            tgt_mask: mask for target sequence self-attention (e.g., to prevent looking ahead).\n",
    "            memory_mask: mask for encoder-decoder attention.\n",
    "            tgt_key_padding_mask: padding mask for target.\n",
    "            memory_key_padding_mask: padding mask for memory.\n",
    "        Returns:\n",
    "            Processed tensor of shape (seq_length, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention in decoder (ensures causal/auto-regressive generation).\n",
    "        attn_output, _ = self.self_attn(tgt, tgt, tgt,\n",
    "                                        attn_mask=tgt_mask,\n",
    "                                        key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(attn_output)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder (cross) attention.\n",
    "        attn_output, _ = self.multihead_attn(tgt, memory, memory,\n",
    "                                             attn_mask=memory_mask,\n",
    "                                             key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(attn_output)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward network.\n",
    "        ff_output = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(ff_output)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "# Quick test of the decoder block with dummy data:\n",
    "decoder_block = TransformerDecoderBlock(d_model, nhead, dim_feedforward, dropout)\n",
    "dummy_tgt = torch.rand(seq_length, batch_size, d_model)\n",
    "# For testing, we can use the encoder output from the previous cell as 'memory'\n",
    "decoder_output = decoder_block(dummy_tgt, encoder_output)\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052c5bc-f49c-4cf4-863a-70ff6571580c",
   "metadata": {},
   "source": [
    "#### Define a Helper Function for Target Masking and Integrate Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10110c45-26e2-40b8-bcf4-01fbcb955c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory (encoder output) shape: torch.Size([10, 32, 512])\n",
      "Final Decoder output shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a square mask for the sequence.\n",
    "    The masked positions are filled with -inf.\n",
    "    Unmasked positions are filled with 0.\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Define sequence lengths and create dummy input data.\n",
    "src_seq_length = 10  # source sequence length\n",
    "tgt_seq_length = 10  # target sequence length\n",
    "\n",
    "dummy_src = torch.rand(src_seq_length, batch_size, d_model)  # (L, N, E)\n",
    "dummy_tgt = torch.rand(tgt_seq_length, batch_size, d_model)  # (L, N, E)\n",
    "\n",
    "# Create target mask to prevent the decoder from looking ahead.\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_seq_length)\n",
    "\n",
    "# Pass through the encoder block.\n",
    "memory = encoder_block(dummy_src)\n",
    "print(\"Memory (encoder output) shape:\", memory.shape)\n",
    "\n",
    "# Pass through the decoder block using the encoder's output as memory.\n",
    "decoder_output = decoder_block(dummy_tgt, memory, tgt_mask=tgt_mask)\n",
    "print(\"Final Decoder output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc032ae9-f759-43e4-b0bb-d6052bb80160",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3af94-9f08-4cf5-b50d-468e4a17e794",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6767e0a-cdd9-4f54-9609-8aacf6d96b54",
   "metadata": {},
   "source": [
    "+ Layer normalization is a technique used to stabilize and accelerate the training of deep neural networks by **normalizing the inputs to each layer**.\n",
    "+ It is particularly well-suited for models like Transformers, where inputs across time steps or sequence positions are processed simultaneously.\n",
    "+ Layer normalization is a normalization method that standardizes the inputs to a neural network layer across the features for each individual sample rather than across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a73a64-413b-459d-81eb-599a7bc16ff6",
   "metadata": {},
   "source": [
    "#### Key Characteristics\n",
    "##### 1. Normalization Across Features:\n",
    "+ Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes the activations within a layer across the feature dimension.\n",
    "+ This means that for each sample (or time step in a sequence), the normalization is performed over all hidden units in that layer.\n",
    "\n",
    "##### 2. Independence from Batch Size:\n",
    "+ Because the normalization is done per sample, layer normalization works well even with very small batch sizes or in settings where the batch size is 1 (such as in some reinforcement learning or online learning scenarios).\n",
    "\n",
    "##### 3. Simplicity in Recurrent and Transformer Models:\n",
    "+ In models that handle sequences—like RNNs and Transformers—layer normalization helps stabilize hidden state dynamics over time and improves convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180b32a-e02a-4cef-8b7f-3b0443807dbb",
   "metadata": {},
   "source": [
    "### Benefits of Layer Normalization\n",
    "#### 1. Stabilizes Training:\n",
    "+ By normalizing the inputs to each layer, the gradients during backpropagation become more stable, helping the network converge faster and more reliably.\n",
    "\n",
    "#### 2. Handles Variable Batch Sizes:\n",
    "+ Since the normalization is done per sample, it is less sensitive to the batch size.\n",
    "+ This makes it a robust choice when training on small batches or even on a single sample at a time.\n",
    "\n",
    "#### 3. Ideal for Sequence Models:\n",
    "+ In tasks where the batch dimension may not be consistent—such as in natural language processing (NLP) tasks with varying sequence lengths—layer normalization is more appropriate than batch normalization.\n",
    "\n",
    "#### 4. Improves Generalization:\n",
    "+ Normalizing activations can reduce internal covariate shift (i.e., the change in the distribution of network activations during training), which in turn can help the network generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf16d7-961c-40c5-8b77-83dd69a086f5",
   "metadata": {},
   "source": [
    "### Use in Transformer Models\n",
    "+ In Transformer architectures, layer normalization is applied at several points:\n",
    "\n",
    "#### Within Encoder and Decoder Blocks:\n",
    "+ After the multi-head self-attention and feed-forward sub-layers, layer normalization is used (often in combination with residual connections) to ensure that the inputs to subsequent layers are well-behaved.\n",
    "\n",
    "#### Before Attention Mechanisms:\n",
    "+ Some variants of the Transformer apply layer normalization before the attention layers (known as pre-norm Transformers) to further improve training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb1649-be4e-42df-933d-8bcc0694822c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14480422-a1fd-441f-948e-e47eda3de4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108e59690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0e5ecc-f069-47c1-b1f8-cd7228aea4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['hello', 'world', 'this', 'is', 'a', 'test']\n",
      "Vocabulary: {'hello': 0, 'world': 1, 'this': 2, 'is': 3, 'a': 4, 'test': 5}\n",
      "Token Indices: [0, 1, 2, 3, 4, 5]\n",
      "Input Indices Tensor:\n",
      " tensor([[0, 1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prepare a simple NLP example\n",
    "\n",
    "# Define a simple sentence and tokenize it.\n",
    "sentence = \"hello world this is a test\".split()\n",
    "print(\"Tokenized Sentence:\", sentence)\n",
    "\n",
    "# Build a small vocabulary from the sentence.\n",
    "# (In practice, you would have a larger vocabulary and better tokenization.)\n",
    "vocab = {word: idx for idx, word in enumerate(sentence)}\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert tokens to indices.\n",
    "indices = [vocab[word] for word in sentence]\n",
    "print(\"Token Indices:\", indices)\n",
    "\n",
    "# Convert indices to a tensor of shape (batch_size, sequence_length).\n",
    "# Here, we assume a batch size of 1.\n",
    "input_indices = torch.tensor([indices])  # shape: (1, sequence_length)\n",
    "print(\"Input Indices Tensor:\\n\", input_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef477c8-4c81-4111-8ce8-770a690343f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Embeddings:\n",
      " tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
      "          -1.6047],\n",
      "         [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,\n",
      "           0.7624],\n",
      "         [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,\n",
      "           1.6806],\n",
      "         [ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,\n",
      "           0.8599],\n",
      "         [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057,\n",
      "          -0.7746],\n",
      "         [-1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347,\n",
      "          -0.4879]]], grad_fn=<EmbeddingBackward0>)\n",
      "Embeddings Shape: torch.Size([1, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create an Embedding Layer and Apply it\n",
    "\n",
    "# Set the embedding dimension.\n",
    "embedding_dim = 8  # You can choose any dimension\n",
    "\n",
    "# Create an embedding layer (vocab_size x embedding_dim).\n",
    "vocab_size = len(vocab)\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get embeddings for the input indices.\n",
    "embeddings = embedding(input_indices)  # shape: (batch_size, sequence_length, embedding_dim)\n",
    "print(\"\\nRaw Embeddings:\\n\", embeddings)\n",
    "\n",
    "# Optionally, view the shape\n",
    "print(\"Embeddings Shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed5bd860-16bf-4fa2-82e5-a327473c1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Embeddings:\n",
      " tensor([[[ 1.3737,  1.0601,  0.6418, -1.5020,  0.4833, -0.8809, -0.0312,\n",
      "          -1.1448],\n",
      "         [-0.5176,  2.0823, -0.1281, -1.2231, -0.4913, -0.3089, -0.5357,\n",
      "           1.1225],\n",
      "         [ 1.2722, -0.7856, -1.1714, -0.1013, -1.4692,  0.6281,  0.3112,\n",
      "           1.3160],\n",
      "         [ 1.0335,  1.0605, -0.0108,  1.1203, -1.3260, -0.8990, -1.3572,\n",
      "           0.3787],\n",
      "         [-1.3584, -0.7856, -0.0628,  2.1023,  0.5421, -0.2872,  0.5274,\n",
      "          -0.6778],\n",
      "         [-1.0002,  1.1404, -0.4319, -0.1983, -0.7626,  2.0854, -0.7294,\n",
      "          -0.1034]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Normalized Embeddings Shape: torch.Size([1, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Apply Layer Normalization on the Embeddings\n",
    "\n",
    "# Layer normalization is typically applied over the embedding dimension.\n",
    "# For each token in the sequence, we want to normalize its embedding vector.\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "# Apply layer normalization to the embeddings.\n",
    "# Note: LayerNorm expects the normalized shape to match the last dimension.\n",
    "normalized_embeddings = layer_norm(embeddings)\n",
    "print(\"\\nNormalized Embeddings:\\n\", normalized_embeddings)\n",
    "\n",
    "# View the shape of the normalized embeddings\n",
    "print(\"Normalized Embeddings Shape:\", normalized_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37297e0-d83c-4251-9a94-9ec2a2e7dcc0",
   "metadata": {},
   "source": [
    " -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3304f4d-0453-4e64-abce-faf14ea68294",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks\n",
    "+ Feed-forward networks (FFNs) are a critical component in Transformer architectures.\n",
    "+ They are applied independently and identically to each position (or token) in the sequence, providing a way to further process and transform the output of the attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601f38f-ba83-4424-8d98-2e1352ef8500",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks in Transformers\n",
    "+ In the context of Transformer models, a feed-forward network is typically a two-layer multilayer perceptron (MLP) applied to each position in the sequence independently.\n",
    "+ The key idea is to further process the token representations after the self-attention mechanism has captured relationships among tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79edf2e-6831-4173-ace0-4681a76c11c0",
   "metadata": {},
   "source": [
    "### Key Characteristics\n",
    "#### 1. Position-wise Application:\n",
    "+ The same feed-forward network is applied to each token separately.\n",
    "+ This means that while the network is shared across all positions, it operates independently for each position, allowing parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe32da-28fc-40b1-b772-22259df74a1a",
   "metadata": {},
   "source": [
    "#### 2. Two-Layer Structure:\n",
    "+ A standard FFN in a Transformer consists of:\n",
    "\n",
    "+ 1. A linear layer that projects the input from the model's hidden size to a higher-dimensional space\n",
    "+ 2. A non-linear activation function (commonly ReLU or GELU) to introduce non-linearity.\n",
    "+ 3. A second linear layer that projects the output back to the hidden size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c64b4-4187-4be9-8b8f-45d7f9777f81",
   "metadata": {},
   "source": [
    "### Why Use Feed-Forward Networks?\n",
    "##### 1. Non-Linearity:\n",
    "+ The introduction of a non-linear activation function allows the network to capture complex patterns in the data that linear transformations alone cannot capture.\n",
    "\n",
    "##### 2. Dimensional Expansion:\n",
    "+ By projecting to a higher-dimensional space, the network can model more complex interactions among features. Then, projecting back to the original space allows these enhanced representations to be integrated back into the overall model.\n",
    "\n",
    "##### 3. Position-wise Independence:\n",
    "+ Since the FFN is applied to each token independently, it does not mix information across different positions. This complements the attention mechanism, which is responsible for mixing information across positions.\n",
    "\n",
    "##### 4. Efficiency:\n",
    "+ The feed-forward network is applied in parallel across tokens, making it computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed2d6e-31c4-412a-99a6-70a277ccbf64",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf94c5b-6a6c-47f9-a6de-0e9aad7a5a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "FFN output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: The input and output dimensionality (model hidden size).\n",
    "            d_ff: The inner-layer dimensionality (feed-forward dimension).\n",
    "            dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()  # Alternatively, use nn.GELU() for smoother activations.\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the first linear transformation\n",
    "        x = self.linear1(x)\n",
    "        # Apply non-linearity\n",
    "        x = self.activation(x)\n",
    "        # Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        # Project back to d_model\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Quick test of the FFN module with dummy data:\n",
    "d_model = 512       # Hidden size\n",
    "d_ff = 2048         # Feed-forward size\n",
    "batch_size = 2      # Number of sequences in the batch\n",
    "seq_length = 10     # Length of each sequence\n",
    "\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "dummy_input = torch.rand(batch_size, seq_length, d_model)\n",
    "ffn_output = ffn(dummy_input)\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"FFN output shape:\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df130aa-91f2-4981-a4be-987cceb449ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: tensor([[0, 1, 2, 3, 4, 5]])\n",
      "Raw embeddings:\n",
      " tensor([[[ 0.4575,  0.9477,  1.7342,  ...,  0.0348,  1.3952, -0.3006],\n",
      "         [-0.4981, -2.1107,  1.0544,  ...,  0.2418, -0.7503, -1.9291],\n",
      "         [-1.0991,  0.0943, -0.2119,  ..., -0.3632,  0.4274,  0.7224],\n",
      "         [ 1.2295,  0.2509,  0.1474,  ..., -0.9729,  1.7464,  0.6245],\n",
      "         [ 0.9247, -0.4319,  0.9695,  ...,  1.2066, -0.3008, -0.8452],\n",
      "         [-1.0557,  1.2071, -0.0109,  ..., -1.6180,  0.1539,  1.0028]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "FFN output:\n",
      " tensor([[[-0.0936,  0.1393, -0.1026,  ...,  0.0180,  0.0283, -0.0821],\n",
      "         [ 0.0056,  0.2733,  0.4768,  ..., -0.0036,  0.2604,  0.1568],\n",
      "         [-0.1127,  0.3853,  0.4542,  ..., -0.2403,  0.0419,  0.5705],\n",
      "         [-0.0903, -0.0647,  0.1574,  ...,  0.0601, -0.6056, -0.0466],\n",
      "         [-0.3474,  0.0300, -0.0115,  ..., -0.0618,  0.3574, -0.1310],\n",
      "         [-0.2085, -0.0121, -0.0578,  ...,  0.1855, -0.1635,  0.0790]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "FFN output shape: torch.Size([1, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: An example using word embeddings\n",
    "\n",
    "# Sample sentence tokenization (for illustration)\n",
    "sentence = \"hello world this is a test\".split()\n",
    "vocab = {word: idx for idx, word in enumerate(sentence)}\n",
    "indices = [vocab[word] for word in sentence]\n",
    "\n",
    "# Create a tensor of token indices (batch size = 1, sequence length = len(sentence))\n",
    "input_indices = torch.tensor([indices])\n",
    "print(\"Token indices:\", input_indices)\n",
    "\n",
    "# Create an embedding layer and obtain embeddings.\n",
    "embedding_dim = d_model  # Must match d_model of FFN\n",
    "embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "embeddings = embedding(input_indices)  # Shape: (1, sequence_length, embedding_dim)\n",
    "print(\"Raw embeddings:\\n\", embeddings)\n",
    "\n",
    "# Apply the Feed-Forward Network on the embeddings.\n",
    "ffn_output = ffn(embeddings)\n",
    "print(\"\\nFFN output:\\n\", ffn_output)\n",
    "print(\"FFN output shape:\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f8494-3daf-4ea3-81fb-a4f51e5e7f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
