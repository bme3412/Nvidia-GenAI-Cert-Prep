{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678f509d-7443-4c4d-b513-6210873d47af",
   "metadata": {},
   "source": [
    "### Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4c6d6-689e-4f42-b12b-a84f25204cf6",
   "metadata": {},
   "source": [
    "#### 1. Self-Attention Mechanism\n",
    "+ 1. The self-attention mechanism is the fundamental building block of the Transformer architecture.\n",
    "+ It allows the model to weigh the **importance of different words in a sequence relative to each other**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27d156-6309-42cb-a2ac-8620b57f55dc",
   "metadata": {},
   "source": [
    "+ 2. The **self-attention mechanism** is a method for relating different positions of a single sequence to compute a representation of the sequence.\n",
    "+ Unlike traditional recurrent or convolutional models, self-attention **allows a model to weigh the importance of different words** (or tokens) in the input sequence relative to each other, regardless of their distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c191b-6fa9-4ee5-8c7f-baeccc5e10f0",
   "metadata": {},
   "source": [
    "+ 3. **Dynamic Weighting**: Each token in the sequence gathers contextual information by “attending” to other tokens.\n",
    "+ Instead of having a fixed-size window (as in convolutions) or sequential dependency (as in RNNs), self-attention dynamically computes the relevance of every token to every other token\n",
    "+ **Parallel Computation**: Since the attention operation can be applied to all tokens simultaneously, it offers significant efficiency improvements over sequential models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53add75d-1318-4ce6-b64d-b9f7fb91fe19",
   "metadata": {},
   "source": [
    "#### Why Is Self-Attention Important?\n",
    "+ Context-Aware Representations: Each token’s representation is informed by the entire sequence, allowing the model to capture long-range dependencies.\n",
    "+ Parallel Processing: Self-attention allows the model to process all tokens simultaneously, significantly improving computational efficiency.\n",
    "+ Flexibility: It can be easily extended to multi-head attention (where several self-attention operations run in parallel), allowing the model to capture diverse types of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f183a0-3399-4605-8e08-5e7515898d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(X, W_Q, W_K, W_V):\n",
    "    # Linear projections\n",
    "    Q = np.dot(X, W_Q)\n",
    "    K = np.dot(X, W_K)\n",
    "    V = np.dot(X, W_V)\n",
    "    \n",
    "    # Compute attention scores (scaled dot-product)\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Multiply weights by values\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Example usage:\n",
    "# X: input embeddings (e.g., shape (sequence_length, embedding_dim))\n",
    "# W_Q, W_K, W_V: randomly initialized or learned weight matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd41c-c226-4384-9be5-1de8050326d6",
   "metadata": {},
   "source": [
    " ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c762150-7f84-44af-97d2-caee2c742076",
   "metadata": {},
   "source": [
    "#### Mathematical Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed747d-6f42-4dda-94f1-914e551d5c2b",
   "metadata": {},
   "source": [
    "##### 1. The Query, Key, and Value Matrices\n",
    "+ For each token in the input sequence, three vectors are computed:\n",
    "+ Query (Q); Key (K); Value (V)\n",
    "\n",
    "##### 2. The self-attention score between a pair of tokens:\n",
    "+ Dot-Product Similarity: Compute the dot product between a token’s query vector and every other token’s key vector.\n",
    "+ Scaling: Divide the result to prevent the dot products from growing too large, which could push the softmax into regions with very small gradients.\n",
    "+ Softmax: Apply the softmax function to obtain a probability distribution over the tokens.\n",
    "\n",
    "##### 3. 3. Interpretation\n",
    "+ Dot-Product: Captures how well the current token (via its query) aligns with other tokens (via their keys).\n",
    "+ Scaling: Helps stabilize gradients during training.\n",
    "+ Softmax: Converts the raw scores into a normalized weight distribution that sums to 1, indicating the importance of each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed6160-26cf-457a-aea4-53fa6d7bfb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46649cfa-e56f-4386-bab7-8b49f5379a6e",
   "metadata": {},
   "source": [
    "#### 2. Multi-Head Attention\n",
    "+ Multi-head attention allows the model to jointly attend to information from different representation subspaces.\n",
    "+ Instead of performing a single attention function, the **model performs multiple attention operations in parallel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41811fcc-3221-41f9-a7b6-6a65b346eb9a",
   "metadata": {},
   "source": [
    "+ Multi-head attention is a critical extension of the self-attention mechanism in transformer architectures.\n",
    "+ Instead of performing a single attention function, multi-head attention **runs multiple attention operations in parallel**, allowing the model to capture diverse relationships and interactions from different subspaces of the input representation.\n",
    "+ **Combining the outputs from multiple attention heads** results in a richer and more robust representation, as the model can consider information from various subspaces.\n",
    "+ Similar to self-attention, multi-head attention is **highly parallelizable**, which benefits computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7453e93-b6f0-460d-ae4a-3bd6a0dc7a27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m W_V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(embedding_dim, embedding_dim)\n\u001b[1;32m     52\u001b[0m W_O \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(embedding_dim, embedding_dim)\n\u001b[0;32m---> 54\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_V\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_O\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-Head Attention Output Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mmulti_head_attention\u001b[0;34m(X, W_Q, W_K, W_V, W_O, num_heads)\u001b[0m\n\u001b[1;32m     27\u001b[0m V \u001b[38;5;241m=\u001b[39m linear_projection(X, W_V)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Apply scaled dot-product attention for each head\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Here, we assume that the attention function can work in batch mode across heads\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m attention_output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_heads, seq_length, head_dim)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Concatenate heads: rearrange to (batch_size, seq_length, embedding_dim)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m concat_attention \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_length, embedding_dim)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscaled_dot_product_attention\u001b[39m(Q, K, V):\n\u001b[1;32m      8\u001b[0m     d_k \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m     scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(Q, \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m     10\u001b[0m     weights \u001b[38;5;241m=\u001b[39m softmax(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(weights, V)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    output = np.matmul(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):\n",
    "    batch_size, seq_length, embedding_dim = X.shape\n",
    "    head_dim = embedding_dim // num_heads\n",
    "\n",
    "    # Linear projections and reshape for each head\n",
    "    def linear_projection(X, W):\n",
    "        proj = np.matmul(X, W)  # (batch_size, seq_length, embedding_dim)\n",
    "        proj = proj.reshape(batch_size, seq_length, num_heads, head_dim)\n",
    "        # Rearrange dimensions: (batch_size, num_heads, seq_length, head_dim)\n",
    "        return proj.transpose(0, 2, 1, 3)\n",
    "    \n",
    "    Q = linear_projection(X, W_Q)\n",
    "    K = linear_projection(X, W_K)\n",
    "    V = linear_projection(X, W_V)\n",
    "\n",
    "    # Apply scaled dot-product attention for each head\n",
    "    # Here, we assume that the attention function can work in batch mode across heads\n",
    "    attention_output, _ = scaled_dot_product_attention(Q, K, V)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "    # Concatenate heads: rearrange to (batch_size, seq_length, embedding_dim)\n",
    "    concat_attention = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "    # Final linear projection\n",
    "    output = np.matmul(concat_attention, W_O)  # (batch_size, seq_length, embedding_dim)\n",
    "    return output\n",
    "\n",
    "# Example dimensions and initialization (for demonstration)\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "embedding_dim = 64\n",
    "num_heads = 8\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Randomly initialize input and weight matrices\n",
    "X = np.random.rand(batch_size, seq_length, embedding_dim)\n",
    "W_Q = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_K = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_V = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_O = np.random.rand(embedding_dim, embedding_dim)\n",
    "\n",
    "output = multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads)\n",
    "print(\"Multi-Head Attention Output Shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b60e2f-402f-4be7-874d-4edcef1f71e3",
   "metadata": {},
   "source": [
    "#### 3. Positional Encoding\n",
    "+ Since the Transformer doesn't use recurrence or convolution, we need to inject information about the relative or absolute position of tokens in the sequence.\n",
    "+ Positional encoding adds this information directly to the embeddings\n",
    "\n",
    "+ Transformer models, unlike recurrent neural networks (RNNs) or convolutional neural networks (CNNs), do **not inherently process data in a sequential manner**.\n",
    "+ They treat input tokens as a set rather than a sequence, meaning that they lack information about the order of tokens.\n",
    "+ **Positional encoding** is introduced to inject this order information into the model. It allows the transformer to **understand the relative and absolute positions of tokens in a sequence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49c2517-81c4-4964-88d5-d8e3c6e5e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Shape: (50, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    Compute the angles for the positional encoding.\n",
    "    pos: position (scalar or array of positions)\n",
    "    i: dimension index (scalar or array of indices)\n",
    "    d_model: dimension of the model embeddings\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate a matrix of positional encodings.\n",
    "    max_len: maximum sequence length\n",
    "    d_model: dimension of the model embeddings\n",
    "    \"\"\"\n",
    "    # Create a matrix of shape (max_len, d_model) with positions and dimensions\n",
    "    pos = np.arange(max_len)[:, np.newaxis]  # Shape (max_len, 1)\n",
    "    i = np.arange(d_model)[np.newaxis, :]     # Shape (1, d_model)\n",
    "    \n",
    "    angle_rads = get_angles(pos, i, d_model)\n",
    "    \n",
    "    # Apply sine to even indices (0, 2, 4, ...) and cosine to odd indices (1, 3, 5, ...)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return angle_rads\n",
    "\n",
    "# Example usage:\n",
    "max_len = 50        # Maximum sequence length\n",
    "d_model = 512       # Embedding dimension\n",
    "\n",
    "pos_encoding = positional_encoding(max_len, d_model)\n",
    "print(\"Positional Encoding Shape:\", pos_encoding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce770297-bd64-4d7a-8c29-c48bf1538d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
