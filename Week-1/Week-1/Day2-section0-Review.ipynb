{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271100a9-f11c-41ae-93e2-c0603cde940f",
   "metadata": {},
   "source": [
    "#### Review of prior day (day 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94586b93-380e-4448-9473-3275db5d5c6d",
   "metadata": {},
   "source": [
    "Yesterday kicked off the process by using Anthropic Claude Sonnet 3.5 to suggest a 30 day study guide schedule based on content from the NVDA Generative AI associate certification program.\n",
    "+ Day 1, in addition to creating file directory as well as `Week1_StudyGuide.ipynb`, started with Neural Network Architecture basics\n",
    "    + Key components for neural nets are: layers, nodes, weights and biases, activation functions and loss functions\n",
    "    + paraphrasing here, but: data is input into the input layer, where data is transformed and introduce to non-linearity via activation functions in the hidden layers, and predicated/inference data is received via the output layer. Across all layers, nodes are activated, weights and biases are updated according to metrics from a loss function, and the process is repeated - data sent into an input layer, nodes are activated as data is transformed in the hidden layers and weights and biases are udpated based on the output of a loss function (difference between predicated and truve values), and then the process repeats for as many loops or epochs of data.\n",
    "    + There are 4 main types of Neural Nets: **Feedforward** (data goes 1 direction); **convolutional** (good for processing grid data such as images); **recurrent** (maintains memory of prior inputs to make predictions); **generative adversarial** (2 networks compete against each other; good for synthetic data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759017-a10f-495a-9193-73ae6d86a355",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01810a-3fa3-4a52-ba8d-f6bd23193bcb",
   "metadata": {},
   "source": [
    "+ Day 1, Section 2 - I read about forward and backpropagation:\n",
    "    + By inputting the data to make predictions, then to keep iterating over the data, as if on a loop, and for every single inference that is made, the output of a loss function is used to then update weights and biases of the parameters for each pass through the data. **Forward Propagation** involves data/nodes being activated while passing through layers of the neural net to make predictions, and **Back Propagation** involves the data/nodes to update assumptions incorporated into weights and biases, and from there the nodes are then once again forward propagated to make predictions,for as long as the program instructs, with each set of anaylsis incorporating predicted data to be backpropagated which then updated weights and biases.\n",
    "    + There is a little bith of math - backpropagation involves computing gradients for all weights in the network, invokes the chain rule of calculus to compute how much each weight contributed to the loss error (compares predicted output vs actual values) for each iteration. The goal of backpropagation is to minimize errors across iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0f93f-58f6-4165-8766-77756f6cff98",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72494810-758f-4797-8c6e-17ae1e5d1cb3",
   "metadata": {},
   "source": [
    "+ Day 1, Section 3 - explored activation functions:\n",
    "    + Activation functions **introduce non-linearity** into the model, which enables it to learn more about the data - most real-world data relationships are non-linear\n",
    "    + some of the activation functions include: binary step and linear activation (both linear functions, primarily used for regression tasks); but also non-linear activation functions such as: ReLu, Sigmoid, Softmax, Tanh and Leaky ReLU, among others.\n",
    "    + ReLU and its variantes are efficent in training deep networks with hidden layers\n",
    "    + For output layers, Sigmoid is used for binary classification, and Softmax for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425186b-0671-4252-94c5-6c9d89766277",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f0fc6-7be3-4e3d-b15f-b9f04072dd32",
   "metadata": {},
   "source": [
    "+ Day 1, Section 4 - on to **loss functions** and **optimization**\n",
    "    + these components work together to improve model performance. Loss functions measure the difference between model-predicted and actual values and quantify into a loss metric to be minimized during training\n",
    "    + Optimization algorithms adjust the neural network's parameters (weights and biases) to minimize the loss function. This includes: **gradient descent** and its variants (includes stochastic and mini-batch), etc.\n",
    "    + Learning rates can be altered as well. But the optimization algorithm is used to update the weights and biases based off the gradient of the parameters for the loss function that is the result of data propagation (forward and back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c98903-9794-4d25-b3c6-9b3d303a58f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
