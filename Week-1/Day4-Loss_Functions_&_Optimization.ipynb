{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7318e4-9209-43c3-91b1-3a63fd8ad457",
   "metadata": {},
   "source": [
    "### Loss Functions & Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52cd7c-c0c0-435d-ac7a-7551a7ac5502",
   "metadata": {},
   "source": [
    "#### 30,000 Foot View:\n",
    "Loss functions and optimization are crucial components in training neural networks, working together to improve the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435dba0-cb01-4e12-a673-99913e4ed172",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88787d-eca0-4718-9929-95fcf0119450",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cbf59-8dc5-4873-916b-f257c908c1b8",
   "metadata": {},
   "source": [
    "+ Loss functions measure the difference between a neural network's predictions and the actual target values.\n",
    "+ They quantify the model's error, providing a metric to be minimized during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84ecc7-474d-4390-b9a0-a94ff8a4af6b",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc29a20-8193-4bf0-b78b-ed32ccffd575",
   "metadata": {},
   "source": [
    "#### Types of Loss Functions\n",
    "+ 1. **Regression loss functions**\n",
    "      + Mean Squared Error (MSE)\n",
    "      + Mean Absolute Error (MAE)\n",
    "\n",
    "+ 2. **Classification loss functions**\n",
    "        + Binary Cross-Entropy\n",
    "        + Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95260c16-3e3b-4c1a-8cb2-163b89428769",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214f401-5ec8-412c-97e4-b3ffad2daf00",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "+ Optimization algorithms adjust the neural network's parameters (weights and biases) to minimize the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af513e15-0f50-4fe8-8aef-417f13f29639",
   "metadata": {},
   "source": [
    "##### Common Optimization Algorithms\n",
    "+ 1. **Gradient Descent**: The most basic optimization algorithm, calculating the direction to adjust weights to reach a minimum\n",
    "  2. **Stochastic Gradient Descent (SGD)**: Updates parameters using a single training example at a time, making it faster and more suitable for large datasets\n",
    "  3. **Mini-Batch Gradient Descent**: A compromise between full-batch and stochastic gradient descent, using small batches of training data\n",
    "  4. **Adam (Adaptive Moment Estimation)**: Combines ideas from other optimizers, adapting learning rates for each parameter and generally performing well across various tasks\n",
    "  5. **RMSProp**: Addresses the diminishing learning rates problem of AdaGrad, making it effective for non-stationary objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac1eb3-c653-4212-8dab-e56b5af71efe",
   "metadata": {},
   "source": [
    "##### Optimization process\n",
    "+ Involves:\n",
    "+ 1. Forward propagation to make predictions;\n",
    "  2. calculating the loss using the chosen function\n",
    "  3. back propagation to compute gradients\n",
    "  4. Updating parameters, using the selected optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5528951-9879-4bd0-8622-b46b13bdb919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
