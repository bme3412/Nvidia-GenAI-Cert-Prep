{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e7097d-85ff-432c-875d-bd4b2c261eea",
   "metadata": {},
   "source": [
    "### Forward & Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44d6e3-a925-4496-a0e3-cee32f859731",
   "metadata": {},
   "source": [
    "#### 30,000 Foot View:\n",
    "+ Forward and backward propagation are fundamental processes in training neural networks, enabling them to learn from data and improve their predictions.\n",
    "+ **Forward Propagation** generates predicitons based on input data; **backward propagation** adjusts model parameters to reduce prediction errors, leading to improved models over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a3fe-b041-4e35-9a9c-521423562c26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8d3b4-f282-4564-b343-d3c62dffb604",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65749a5b-e160-4edb-94ca-940f4d2c6fa7",
   "metadata": {},
   "source": [
    "##### Definition:\n",
    "+ Forward Propagation is the process of passing input data through the layers of a NN to get an output\n",
    "+ This involves calculating the weighted sum of inputs at eaach neuron, and applying an activation function, to prdouce outputs for the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aad24d-2a7e-4e9a-bd84-76e4254c3ab7",
   "metadata": {},
   "source": [
    "##### Steps in Forward Propagation:\n",
    "+ Input Layer: The input data is fed into the network\n",
    "  \n",
    "+ Weighted Sum Calculation: Each neuron computes a weighted sum of its inputs, taking into account each input and its weights and biases\n",
    "\n",
    "  \n",
    "+ Activation Function: The weighted sum is passed through an activation function (like ReLU, Tahn or Sigmoid) to introduce non-linearity\n",
    "\n",
    "  \n",
    "+ Output Layer: This process continues through hidden layers until reaching the output layer, which produces the final output\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70b5bb-6612-446a-a7ca-c21cf82cd15f",
   "metadata": {},
   "source": [
    "##### Importance:\n",
    "+ Forward Propagation is crucial for both training and inference. During training, it generates predictions that are compared with actual outputs to compute error; while for inference it is used to make new predictions on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11f580-5fd6-4f1f-bcff-86c7a0572731",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59f1e0-29db-46a6-a54c-990623ba714e",
   "metadata": {},
   "source": [
    "#### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833ddef-4756-447c-867a-e7de742394dc",
   "metadata": {},
   "source": [
    "##### Definition:\n",
    "+ Backward propagation, or backpropagation, is a method used to optimize neural networks by calculating gradients of the loss function with respect to each weight and bias in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6f185-7b61-4bdd-98de-63ec855b8d88",
   "metadata": {},
   "source": [
    "#### Steps in Backward Propagation:\n",
    "+ 1. Calculate Error - after forward propagation, the error/(loss) is computed by comparing the predicted output with the actual target values using a loss function\n",
    "+ 2. Gradient Calculation - uses chain rule from calculus, to calculate how much each weight contributed to the error\n",
    "+ 3. Update Weights - weights are then updated using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff1da2-46f1-4c35-9ae2-75e24876c706",
   "metadata": {},
   "source": [
    "##### Importance:\n",
    "+ Backpropagation allows neural networks to learn, by minimizing errors across multiple iterations\n",
    "+ It efficiently compute all gradients for all wights in the network, enabling effective updates that lead to improved model performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b8a03-fd57-42ba-9831-e491290e4a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (PyTorch)",
   "language": "python",
   "name": "torch-ml-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
